{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in one-hot format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmc_forms = []\n",
    "for l in open('gmc_aligned.tsv','r'):\n",
    "    gmc_forms.append(l.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data():\n",
    "    \"\"\"load aligned forms\"\"\"\n",
    "    gmc_forms = []\n",
    "    for l in open('gmc_aligned.tsv','r'):\n",
    "        gmc_forms.append(l.split('\\t'))\n",
    "    \"\"\"\"\"\"\n",
    "    gmc_forms=[l for l in gmc_forms if l[0]=='ang']\n",
    "    x_raw = [l[1].split() for l in gmc_forms]\n",
    "    y_raw = [l[2].split() for l in gmc_forms]\n",
    "    \"\"\"length of set of symbols for input and output\"\"\"\n",
    "    x_alpha = sorted(set([s for w in x_raw for s in w]))\n",
    "    y_alpha = sorted(set([s for w in y_raw for s in w]))\n",
    "    \"\"\"max lengh (for padding)\"\"\"\n",
    "    x_max = max([len(w) for w in x_raw])\n",
    "    y_max = max([len(w) for w in y_raw])\n",
    "    \"\"\"\"\"\"\n",
    "    N = len(x_raw)\n",
    "    \"\"\"\"\"\"\n",
    "    x = np.zeros([N,x_max,len(x_alpha)],dtype='float32')\n",
    "    y = np.zeros([N,y_max,len(y_alpha)],dtype='float32')\n",
    "    \"\"\"\"\"\"\n",
    "    for i,w in enumerate(x_raw):\n",
    "        for j,s in enumerate(w):\n",
    "            x[i,j,x_alpha.index(s)] = 1\n",
    "    \"\"\"\"\"\"\n",
    "    for i,w in enumerate(y_raw):\n",
    "        for j,s in enumerate(w):\n",
    "            y[i,j,y_alpha.index(s)] = 1\n",
    "    cutoff = int(.9*len(x))\n",
    "    x_train = x[:cutoff,:,:]\n",
    "    x_test = x[cutoff:,:,:]\n",
    "    y_train = y[:cutoff,:,:]\n",
    "    y_test = y[cutoff:,:,:]\n",
    "    return(x_train,y_train,x_test,y_test,x_max,y_max,N)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train,y_train,x_test,y_test,x_max,y_max,N = gen_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input symbols: 34\n",
      "Number of output symbols: 40\n",
      "Maximum sequence length: 29\n",
      "Number of string pairs: 5588+621\n"
     ]
    }
   ],
   "source": [
    "X = x_train.shape[2]\n",
    "Y = y_train.shape[2]\n",
    "L = y_train.shape[1]\n",
    "N = x_train.shape[0]\n",
    "Z = 10\n",
    "\n",
    "print('Number of input symbols: {}'.format(X))\n",
    "print('Number of output symbols: {}'.format(Y))\n",
    "print('Maximum sequence length: {}'.format(L))\n",
    "print('Number of string pairs: {}+{}'.format(x_train.shape[0],x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a feed-forward neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = tf.placeholder(tf.float32,shape=(None,L,X))\n",
    "y_ = tf.placeholder(tf.float32,shape=(None,L,Y))\n",
    "U = tf.Variable(tf.random_normal([X,Z]))\n",
    "hidden = tf.nn.softmax(tf.tensordot(x_,U,axes=1))            #L by Z matrix\n",
    "W = tf.Variable(tf.random_normal([Z,Y]))\n",
    "output_layer = tf.tensordot(hidden,W,axes=1)                 #L by Y matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(tf.float32, shape=(), name='learning_rate')\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output_layer, labels=y_))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(output_layer)\n",
    "pred_label = tf.argmax(pred,1)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create operation which will initialize all variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Configure GPU not to use all memory\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Start a new tensorflow session and initialize variables\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss= 1.122533042\n",
      "Epoch: 0002 loss= 1.116518313\n",
      "Epoch: 0003 loss= 1.110571521\n",
      "Epoch: 0004 loss= 1.104687836\n",
      "Epoch: 0005 loss= 1.098867050\n",
      "Epoch: 0006 loss= 1.093107848\n",
      "Epoch: 0007 loss= 1.087408536\n",
      "Epoch: 0008 loss= 1.081767051\n",
      "Epoch: 0009 loss= 1.076181161\n",
      "Epoch: 0010 loss= 1.070648859\n",
      "Epoch: 0011 loss= 1.065168915\n",
      "Epoch: 0012 loss= 1.059741079\n",
      "Epoch: 0013 loss= 1.054365574\n",
      "Epoch: 0014 loss= 1.049042596\n",
      "Epoch: 0015 loss= 1.043771623\n",
      "Epoch: 0016 loss= 1.038551374\n",
      "Epoch: 0017 loss= 1.033379488\n",
      "Epoch: 0018 loss= 1.028253002\n",
      "Epoch: 0019 loss= 1.023169256\n",
      "Epoch: 0020 loss= 1.018126081\n",
      "Epoch: 0021 loss= 1.013122070\n",
      "Epoch: 0022 loss= 1.008156263\n",
      "Epoch: 0023 loss= 1.003227960\n",
      "Epoch: 0024 loss= 0.998336611\n",
      "Epoch: 0025 loss= 0.993481652\n",
      "Epoch: 0026 loss= 0.988662663\n",
      "Epoch: 0027 loss= 0.983879193\n",
      "Epoch: 0028 loss= 0.979130704\n",
      "Epoch: 0029 loss= 0.974416885\n",
      "Epoch: 0030 loss= 0.969737257\n",
      "Epoch: 0031 loss= 0.965091488\n",
      "Epoch: 0032 loss= 0.960479182\n",
      "Epoch: 0033 loss= 0.955900020\n",
      "Epoch: 0034 loss= 0.951353627\n",
      "Epoch: 0035 loss= 0.946839660\n",
      "Epoch: 0036 loss= 0.942357659\n",
      "Epoch: 0037 loss= 0.937907142\n",
      "Epoch: 0038 loss= 0.933487599\n",
      "Epoch: 0039 loss= 0.929098210\n",
      "Epoch: 0040 loss= 0.924737885\n",
      "Epoch: 0041 loss= 0.920405166\n",
      "Epoch: 0042 loss= 0.916098072\n",
      "Epoch: 0043 loss= 0.911814175\n",
      "Epoch: 0044 loss= 0.907550689\n",
      "Epoch: 0045 loss= 0.903304851\n",
      "Epoch: 0046 loss= 0.899074218\n",
      "Epoch: 0047 loss= 0.894856894\n",
      "Epoch: 0048 loss= 0.890651684\n",
      "Epoch: 0049 loss= 0.886458105\n",
      "Epoch: 0050 loss= 0.882276412\n",
      "Epoch: 0051 loss= 0.878107264\n",
      "Epoch: 0052 loss= 0.873951784\n",
      "Epoch: 0053 loss= 0.869811279\n",
      "Epoch: 0054 loss= 0.865687203\n",
      "Epoch: 0055 loss= 0.861581129\n",
      "Epoch: 0056 loss= 0.857494525\n",
      "Epoch: 0057 loss= 0.853428839\n",
      "Epoch: 0058 loss= 0.849385426\n",
      "Epoch: 0059 loss= 0.845365644\n",
      "Epoch: 0060 loss= 0.841370760\n",
      "Epoch: 0061 loss= 0.837401973\n",
      "Epoch: 0062 loss= 0.833460347\n",
      "Epoch: 0063 loss= 0.829546897\n",
      "Epoch: 0064 loss= 0.825662620\n",
      "Epoch: 0065 loss= 0.821808388\n",
      "Epoch: 0066 loss= 0.817985019\n",
      "Epoch: 0067 loss= 0.814193288\n",
      "Epoch: 0068 loss= 0.810433929\n",
      "Epoch: 0069 loss= 0.806707585\n",
      "Epoch: 0070 loss= 0.803014814\n",
      "Epoch: 0071 loss= 0.799356181\n",
      "Epoch: 0072 loss= 0.795732185\n",
      "Epoch: 0073 loss= 0.792143329\n",
      "Epoch: 0074 loss= 0.788590040\n",
      "Epoch: 0075 loss= 0.785072725\n",
      "Epoch: 0076 loss= 0.781591752\n",
      "Epoch: 0077 loss= 0.778147469\n",
      "Epoch: 0078 loss= 0.774740313\n",
      "Epoch: 0079 loss= 0.771370636\n",
      "Epoch: 0080 loss= 0.768038832\n",
      "Epoch: 0081 loss= 0.764745219\n",
      "Epoch: 0082 loss= 0.761490173\n",
      "Epoch: 0083 loss= 0.758273902\n",
      "Epoch: 0084 loss= 0.755096430\n",
      "Epoch: 0085 loss= 0.751957385\n",
      "Epoch: 0086 loss= 0.748855896\n",
      "Epoch: 0087 loss= 0.745790469\n",
      "Epoch: 0088 loss= 0.742759002\n",
      "Epoch: 0089 loss= 0.739759072\n",
      "Epoch: 0090 loss= 0.736787941\n",
      "Epoch: 0091 loss= 0.733842970\n",
      "Epoch: 0092 loss= 0.730921589\n",
      "Epoch: 0093 loss= 0.728021446\n",
      "Epoch: 0094 loss= 0.725140620\n",
      "Epoch: 0095 loss= 0.722277900\n",
      "Epoch: 0096 loss= 0.719432455\n",
      "Epoch: 0097 loss= 0.716604223\n",
      "Epoch: 0098 loss= 0.713793495\n",
      "Epoch: 0099 loss= 0.711000918\n",
      "Epoch: 0100 loss= 0.708227288\n",
      "Epoch: 0101 loss= 0.705473444\n",
      "Epoch: 0102 loss= 0.702740255\n",
      "Epoch: 0103 loss= 0.700028519\n",
      "Epoch: 0104 loss= 0.697339023\n",
      "Epoch: 0105 loss= 0.694672496\n",
      "Epoch: 0106 loss= 0.692029517\n",
      "Epoch: 0107 loss= 0.689410660\n",
      "Epoch: 0108 loss= 0.686816348\n",
      "Epoch: 0109 loss= 0.684247001\n",
      "Epoch: 0110 loss= 0.681703005\n",
      "Epoch: 0111 loss= 0.679184635\n",
      "Epoch: 0112 loss= 0.676692137\n",
      "Epoch: 0113 loss= 0.674225755\n",
      "Epoch: 0114 loss= 0.671785675\n",
      "Epoch: 0115 loss= 0.669372009\n",
      "Epoch: 0116 loss= 0.666984955\n",
      "Epoch: 0117 loss= 0.664624583\n",
      "Epoch: 0118 loss= 0.662290991\n",
      "Epoch: 0119 loss= 0.659984285\n",
      "Epoch: 0120 loss= 0.657704559\n",
      "Epoch: 0121 loss= 0.655451903\n",
      "Epoch: 0122 loss= 0.653226400\n",
      "Epoch: 0123 loss= 0.651028107\n",
      "Epoch: 0124 loss= 0.648857092\n",
      "Epoch: 0125 loss= 0.646713355\n",
      "Epoch: 0126 loss= 0.644596901\n",
      "Epoch: 0127 loss= 0.642507676\n",
      "Epoch: 0128 loss= 0.640445515\n",
      "Epoch: 0129 loss= 0.638410172\n",
      "Epoch: 0130 loss= 0.636401305\n",
      "Epoch: 0131 loss= 0.634418395\n",
      "Epoch: 0132 loss= 0.632460797\n",
      "Epoch: 0133 loss= 0.630527738\n",
      "Epoch: 0134 loss= 0.628618308\n",
      "Epoch: 0135 loss= 0.626731562\n",
      "Epoch: 0136 loss= 0.624866397\n",
      "Epoch: 0137 loss= 0.623021710\n",
      "Epoch: 0138 loss= 0.621196378\n",
      "Epoch: 0139 loss= 0.619389333\n",
      "Epoch: 0140 loss= 0.617599555\n",
      "Epoch: 0141 loss= 0.615826083\n",
      "Epoch: 0142 loss= 0.614068026\n",
      "Epoch: 0143 loss= 0.612324575\n",
      "Epoch: 0144 loss= 0.610595032\n",
      "Epoch: 0145 loss= 0.608878762\n",
      "Epoch: 0146 loss= 0.607175196\n",
      "Epoch: 0147 loss= 0.605483809\n",
      "Epoch: 0148 loss= 0.603804174\n",
      "Epoch: 0149 loss= 0.602135879\n",
      "Epoch: 0150 loss= 0.600478567\n",
      "Epoch: 0151 loss= 0.598831882\n",
      "Epoch: 0152 loss= 0.597195518\n",
      "Epoch: 0153 loss= 0.595569176\n",
      "Epoch: 0154 loss= 0.593952560\n",
      "Epoch: 0155 loss= 0.592345439\n",
      "Epoch: 0156 loss= 0.590747449\n",
      "Epoch: 0157 loss= 0.589158343\n",
      "Epoch: 0158 loss= 0.587577855\n",
      "Epoch: 0159 loss= 0.586005675\n",
      "Epoch: 0160 loss= 0.584441524\n",
      "Epoch: 0161 loss= 0.582885126\n",
      "Epoch: 0162 loss= 0.581336180\n",
      "Epoch: 0163 loss= 0.579794451\n",
      "Epoch: 0164 loss= 0.578259741\n",
      "Epoch: 0165 loss= 0.576731839\n",
      "Epoch: 0166 loss= 0.575210656\n",
      "Epoch: 0167 loss= 0.573696180\n",
      "Epoch: 0168 loss= 0.572188440\n",
      "Epoch: 0169 loss= 0.570687597\n",
      "Epoch: 0170 loss= 0.569193867\n",
      "Epoch: 0171 loss= 0.567707627\n",
      "Epoch: 0172 loss= 0.566229270\n",
      "Epoch: 0173 loss= 0.564759230\n",
      "Epoch: 0174 loss= 0.563298011\n",
      "Epoch: 0175 loss= 0.561846164\n",
      "Epoch: 0176 loss= 0.560404180\n",
      "Epoch: 0177 loss= 0.558972591\n",
      "Epoch: 0178 loss= 0.557551928\n",
      "Epoch: 0179 loss= 0.556142659\n",
      "Epoch: 0180 loss= 0.554745268\n",
      "Epoch: 0181 loss= 0.553360201\n",
      "Epoch: 0182 loss= 0.551987803\n",
      "Epoch: 0183 loss= 0.550628435\n",
      "Epoch: 0184 loss= 0.549282404\n",
      "Epoch: 0185 loss= 0.547950007\n",
      "Epoch: 0186 loss= 0.546631479\n",
      "Epoch: 0187 loss= 0.545327020\n",
      "Epoch: 0188 loss= 0.544036816\n",
      "Epoch: 0189 loss= 0.542760993\n",
      "Epoch: 0190 loss= 0.541499652\n",
      "Epoch: 0191 loss= 0.540252879\n",
      "Epoch: 0192 loss= 0.539020706\n",
      "Epoch: 0193 loss= 0.537803155\n",
      "Epoch: 0194 loss= 0.536600218\n",
      "Epoch: 0195 loss= 0.535411832\n",
      "Epoch: 0196 loss= 0.534237956\n",
      "Epoch: 0197 loss= 0.533078507\n",
      "Epoch: 0198 loss= 0.531933382\n",
      "Epoch: 0199 loss= 0.530802429\n",
      "Epoch: 0200 loss= 0.529685514\n",
      "Epoch: 0201 loss= 0.528582477\n",
      "Epoch: 0202 loss= 0.527493104\n",
      "Epoch: 0203 loss= 0.526417257\n",
      "Epoch: 0204 loss= 0.525354705\n",
      "Epoch: 0205 loss= 0.524305253\n",
      "Epoch: 0206 loss= 0.523268693\n",
      "Epoch: 0207 loss= 0.522244814\n",
      "Epoch: 0208 loss= 0.521233422\n",
      "Epoch: 0209 loss= 0.520234286\n",
      "Epoch: 0210 loss= 0.519247230\n",
      "Epoch: 0211 loss= 0.518272033\n",
      "Epoch: 0212 loss= 0.517308512\n",
      "Epoch: 0213 loss= 0.516356463\n",
      "Epoch: 0214 loss= 0.515415735\n",
      "Epoch: 0215 loss= 0.514486121\n",
      "Epoch: 0216 loss= 0.513567441\n",
      "Epoch: 0217 loss= 0.512659525\n",
      "Epoch: 0218 loss= 0.511762214\n",
      "Epoch: 0219 loss= 0.510875341\n",
      "Epoch: 0220 loss= 0.509998752\n",
      "Epoch: 0221 loss= 0.509132291\n",
      "Epoch: 0222 loss= 0.508275816\n",
      "Epoch: 0223 loss= 0.507429177\n",
      "Epoch: 0224 loss= 0.506592212\n",
      "Epoch: 0225 loss= 0.505764788\n",
      "Epoch: 0226 loss= 0.504946774\n",
      "Epoch: 0227 loss= 0.504138018\n",
      "Epoch: 0228 loss= 0.503338411\n",
      "Epoch: 0229 loss= 0.502547817\n",
      "Epoch: 0230 loss= 0.501766100\n",
      "Epoch: 0231 loss= 0.500993126\n",
      "Epoch: 0232 loss= 0.500228766\n",
      "Epoch: 0233 loss= 0.499472924\n",
      "Epoch: 0234 loss= 0.498725467\n",
      "Epoch: 0235 loss= 0.497986271\n",
      "Epoch: 0236 loss= 0.497255240\n",
      "Epoch: 0237 loss= 0.496532213\n",
      "Epoch: 0238 loss= 0.495817117\n",
      "Epoch: 0239 loss= 0.495109827\n",
      "Epoch: 0240 loss= 0.494410225\n",
      "Epoch: 0241 loss= 0.493718232\n",
      "Epoch: 0242 loss= 0.493033722\n",
      "Epoch: 0243 loss= 0.492356588\n",
      "Epoch: 0244 loss= 0.491686708\n",
      "Epoch: 0245 loss= 0.491023999\n",
      "Epoch: 0246 loss= 0.490368361\n",
      "Epoch: 0247 loss= 0.489719674\n",
      "Epoch: 0248 loss= 0.489077873\n",
      "Epoch: 0249 loss= 0.488442822\n",
      "Epoch: 0250 loss= 0.487814439\n",
      "Epoch: 0251 loss= 0.487192650\n",
      "Epoch: 0252 loss= 0.486577321\n",
      "Epoch: 0253 loss= 0.485968384\n",
      "Epoch: 0254 loss= 0.485365728\n",
      "Epoch: 0255 loss= 0.484769282\n",
      "Epoch: 0256 loss= 0.484178944\n",
      "Epoch: 0257 loss= 0.483594619\n",
      "Epoch: 0258 loss= 0.483016227\n",
      "Epoch: 0259 loss= 0.482443674\n",
      "Epoch: 0260 loss= 0.481876880\n",
      "Epoch: 0261 loss= 0.481315760\n",
      "Epoch: 0262 loss= 0.480760222\n",
      "Epoch: 0263 loss= 0.480210190\n",
      "Epoch: 0264 loss= 0.479665578\n",
      "Epoch: 0265 loss= 0.479126320\n",
      "Epoch: 0266 loss= 0.478592311\n",
      "Epoch: 0267 loss= 0.478063491\n",
      "Epoch: 0268 loss= 0.477539777\n",
      "Epoch: 0269 loss= 0.477021091\n",
      "Epoch: 0270 loss= 0.476507344\n",
      "Epoch: 0271 loss= 0.475998491\n",
      "Epoch: 0272 loss= 0.475494437\n",
      "Epoch: 0273 loss= 0.474995124\n",
      "Epoch: 0274 loss= 0.474500468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0275 loss= 0.474010404\n",
      "Epoch: 0276 loss= 0.473524879\n",
      "Epoch: 0277 loss= 0.473043800\n",
      "Epoch: 0278 loss= 0.472567094\n",
      "Epoch: 0279 loss= 0.472094722\n",
      "Epoch: 0280 loss= 0.471626607\n",
      "Epoch: 0281 loss= 0.471162681\n",
      "Epoch: 0282 loss= 0.470702903\n",
      "Epoch: 0283 loss= 0.470247190\n",
      "Epoch: 0284 loss= 0.469795478\n",
      "Epoch: 0285 loss= 0.469347747\n",
      "Epoch: 0286 loss= 0.468903870\n",
      "Epoch: 0287 loss= 0.468463847\n",
      "Epoch: 0288 loss= 0.468027600\n",
      "Epoch: 0289 loss= 0.467595070\n",
      "Epoch: 0290 loss= 0.467166206\n",
      "Epoch: 0291 loss= 0.466740970\n",
      "Epoch: 0292 loss= 0.466319287\n",
      "Epoch: 0293 loss= 0.465901130\n",
      "Epoch: 0294 loss= 0.465486426\n",
      "Epoch: 0295 loss= 0.465075145\n",
      "Epoch: 0296 loss= 0.464667215\n",
      "Epoch: 0297 loss= 0.464262598\n",
      "Epoch: 0298 loss= 0.463861231\n",
      "Epoch: 0299 loss= 0.463463091\n",
      "Epoch: 0300 loss= 0.463068135\n",
      "Epoch: 0301 loss= 0.462676312\n",
      "Epoch: 0302 loss= 0.462287561\n",
      "Epoch: 0303 loss= 0.461901855\n",
      "Epoch: 0304 loss= 0.461519154\n",
      "Epoch: 0305 loss= 0.461139422\n",
      "Epoch: 0306 loss= 0.460762590\n",
      "Epoch: 0307 loss= 0.460388649\n",
      "Epoch: 0308 loss= 0.460017554\n",
      "Epoch: 0309 loss= 0.459649249\n",
      "Epoch: 0310 loss= 0.459283717\n",
      "Epoch: 0311 loss= 0.458920896\n",
      "Epoch: 0312 loss= 0.458560778\n",
      "Epoch: 0313 loss= 0.458203319\n",
      "Epoch: 0314 loss= 0.457848479\n",
      "Epoch: 0315 loss= 0.457496209\n",
      "Epoch: 0316 loss= 0.457146486\n",
      "Epoch: 0317 loss= 0.456799281\n",
      "Epoch: 0318 loss= 0.456454574\n",
      "Epoch: 0319 loss= 0.456112324\n",
      "Epoch: 0320 loss= 0.455772483\n",
      "Epoch: 0321 loss= 0.455435031\n",
      "Epoch: 0322 loss= 0.455099941\n",
      "Epoch: 0323 loss= 0.454767198\n",
      "Epoch: 0324 loss= 0.454436738\n",
      "Epoch: 0325 loss= 0.454108559\n",
      "Epoch: 0326 loss= 0.453782626\n",
      "Epoch: 0327 loss= 0.453458898\n",
      "Epoch: 0328 loss= 0.453137376\n",
      "Epoch: 0329 loss= 0.452818013\n",
      "Epoch: 0330 loss= 0.452500813\n",
      "Epoch: 0331 loss= 0.452185721\n",
      "Epoch: 0332 loss= 0.451872721\n",
      "Epoch: 0333 loss= 0.451561778\n",
      "Epoch: 0334 loss= 0.451252881\n",
      "Epoch: 0335 loss= 0.450946017\n",
      "Epoch: 0336 loss= 0.450641155\n",
      "Epoch: 0337 loss= 0.450338271\n",
      "Epoch: 0338 loss= 0.450037354\n",
      "Epoch: 0339 loss= 0.449738364\n",
      "Epoch: 0340 loss= 0.449441296\n",
      "Epoch: 0341 loss= 0.449146103\n",
      "Epoch: 0342 loss= 0.448852783\n",
      "Epoch: 0343 loss= 0.448561328\n",
      "Epoch: 0344 loss= 0.448271702\n",
      "Epoch: 0345 loss= 0.447983894\n",
      "Epoch: 0346 loss= 0.447697893\n",
      "Epoch: 0347 loss= 0.447413678\n",
      "Epoch: 0348 loss= 0.447131235\n",
      "Epoch: 0349 loss= 0.446850519\n",
      "Epoch: 0350 loss= 0.446571535\n",
      "Epoch: 0351 loss= 0.446294264\n",
      "Epoch: 0352 loss= 0.446018695\n",
      "Epoch: 0353 loss= 0.445744800\n",
      "Epoch: 0354 loss= 0.445472569\n",
      "Epoch: 0355 loss= 0.445201999\n",
      "Epoch: 0356 loss= 0.444933056\n",
      "Epoch: 0357 loss= 0.444665714\n",
      "Epoch: 0358 loss= 0.444399986\n",
      "Epoch: 0359 loss= 0.444135863\n",
      "Epoch: 0360 loss= 0.443873305\n",
      "Epoch: 0361 loss= 0.443612313\n",
      "Epoch: 0362 loss= 0.443352866\n",
      "Epoch: 0363 loss= 0.443094949\n",
      "Epoch: 0364 loss= 0.442838550\n",
      "Epoch: 0365 loss= 0.442583663\n",
      "Epoch: 0366 loss= 0.442330284\n",
      "Epoch: 0367 loss= 0.442078380\n",
      "Epoch: 0368 loss= 0.441827933\n",
      "Epoch: 0369 loss= 0.441578945\n",
      "Epoch: 0370 loss= 0.441331403\n",
      "Epoch: 0371 loss= 0.441085302\n",
      "Epoch: 0372 loss= 0.440840636\n",
      "Epoch: 0373 loss= 0.440597377\n",
      "Epoch: 0374 loss= 0.440355537\n",
      "Epoch: 0375 loss= 0.440115067\n",
      "Epoch: 0376 loss= 0.439875961\n",
      "Epoch: 0377 loss= 0.439638218\n",
      "Epoch: 0378 loss= 0.439401833\n",
      "Epoch: 0379 loss= 0.439166795\n",
      "Epoch: 0380 loss= 0.438933072\n",
      "Epoch: 0381 loss= 0.438700678\n",
      "Epoch: 0382 loss= 0.438469588\n",
      "Epoch: 0383 loss= 0.438239800\n",
      "Epoch: 0384 loss= 0.438011297\n",
      "Epoch: 0385 loss= 0.437784073\n",
      "Epoch: 0386 loss= 0.437558097\n",
      "Epoch: 0387 loss= 0.437333377\n",
      "Epoch: 0388 loss= 0.437109901\n",
      "Epoch: 0389 loss= 0.436887649\n",
      "Epoch: 0390 loss= 0.436666608\n",
      "Epoch: 0391 loss= 0.436446782\n",
      "Epoch: 0392 loss= 0.436228161\n",
      "Epoch: 0393 loss= 0.436010742\n",
      "Epoch: 0394 loss= 0.435794494\n",
      "Epoch: 0395 loss= 0.435579407\n",
      "Epoch: 0396 loss= 0.435365482\n",
      "Epoch: 0397 loss= 0.435152707\n",
      "Epoch: 0398 loss= 0.434941069\n",
      "Epoch: 0399 loss= 0.434730560\n",
      "Epoch: 0400 loss= 0.434521154\n",
      "Epoch: 0401 loss= 0.434312856\n",
      "Epoch: 0402 loss= 0.434105665\n",
      "Epoch: 0403 loss= 0.433899558\n",
      "Epoch: 0404 loss= 0.433694524\n",
      "Epoch: 0405 loss= 0.433490566\n",
      "Epoch: 0406 loss= 0.433287647\n",
      "Epoch: 0407 loss= 0.433085791\n",
      "Epoch: 0408 loss= 0.432884966\n",
      "Epoch: 0409 loss= 0.432685169\n",
      "Epoch: 0410 loss= 0.432486397\n",
      "Epoch: 0411 loss= 0.432288644\n",
      "Epoch: 0412 loss= 0.432091874\n",
      "Epoch: 0413 loss= 0.431896091\n",
      "Epoch: 0414 loss= 0.431701281\n",
      "Epoch: 0415 loss= 0.431507427\n",
      "Epoch: 0416 loss= 0.431314558\n",
      "Epoch: 0417 loss= 0.431122632\n",
      "Epoch: 0418 loss= 0.430931648\n",
      "Epoch: 0419 loss= 0.430741589\n",
      "Epoch: 0420 loss= 0.430552460\n",
      "Epoch: 0421 loss= 0.430364234\n",
      "Epoch: 0422 loss= 0.430176906\n",
      "Epoch: 0423 loss= 0.429990485\n",
      "Epoch: 0424 loss= 0.429804943\n",
      "Epoch: 0425 loss= 0.429620272\n",
      "Epoch: 0426 loss= 0.429436462\n",
      "Epoch: 0427 loss= 0.429253514\n",
      "Epoch: 0428 loss= 0.429071418\n",
      "Epoch: 0429 loss= 0.428890151\n",
      "Epoch: 0430 loss= 0.428709713\n",
      "Epoch: 0431 loss= 0.428530108\n",
      "Epoch: 0432 loss= 0.428351303\n",
      "Epoch: 0433 loss= 0.428173304\n",
      "Epoch: 0434 loss= 0.427996121\n",
      "Epoch: 0435 loss= 0.427819718\n",
      "Epoch: 0436 loss= 0.427644097\n",
      "Epoch: 0437 loss= 0.427469256\n",
      "Epoch: 0438 loss= 0.427295163\n",
      "Epoch: 0439 loss= 0.427121824\n",
      "Epoch: 0440 loss= 0.426949234\n",
      "Epoch: 0441 loss= 0.426777389\n",
      "Epoch: 0442 loss= 0.426606273\n",
      "Epoch: 0443 loss= 0.426435888\n",
      "Epoch: 0444 loss= 0.426266218\n",
      "Epoch: 0445 loss= 0.426097269\n",
      "Epoch: 0446 loss= 0.425929022\n",
      "Epoch: 0447 loss= 0.425761471\n",
      "Epoch: 0448 loss= 0.425594603\n",
      "Epoch: 0449 loss= 0.425428421\n",
      "Epoch: 0450 loss= 0.425262897\n",
      "Epoch: 0451 loss= 0.425098047\n",
      "Epoch: 0452 loss= 0.424933864\n",
      "Epoch: 0453 loss= 0.424770325\n",
      "Epoch: 0454 loss= 0.424607434\n",
      "Epoch: 0455 loss= 0.424445197\n",
      "Epoch: 0456 loss= 0.424283587\n",
      "Epoch: 0457 loss= 0.424122607\n",
      "Epoch: 0458 loss= 0.423962232\n",
      "Epoch: 0459 loss= 0.423802485\n",
      "Epoch: 0460 loss= 0.423643346\n",
      "Epoch: 0461 loss= 0.423484809\n",
      "Epoch: 0462 loss= 0.423326859\n",
      "Epoch: 0463 loss= 0.423169523\n",
      "Epoch: 0464 loss= 0.423012751\n",
      "Epoch: 0465 loss= 0.422856576\n",
      "Epoch: 0466 loss= 0.422700973\n",
      "Epoch: 0467 loss= 0.422545957\n",
      "Epoch: 0468 loss= 0.422391508\n",
      "Epoch: 0469 loss= 0.422237618\n",
      "Epoch: 0470 loss= 0.422084294\n",
      "Epoch: 0471 loss= 0.421931539\n",
      "Epoch: 0472 loss= 0.421779344\n",
      "Epoch: 0473 loss= 0.421627682\n",
      "Epoch: 0474 loss= 0.421476571\n",
      "Epoch: 0475 loss= 0.421325992\n",
      "Epoch: 0476 loss= 0.421175948\n",
      "Epoch: 0477 loss= 0.421026437\n",
      "Epoch: 0478 loss= 0.420877442\n",
      "Epoch: 0479 loss= 0.420728976\n",
      "Epoch: 0480 loss= 0.420581033\n",
      "Epoch: 0481 loss= 0.420433615\n",
      "Epoch: 0482 loss= 0.420286707\n",
      "Epoch: 0483 loss= 0.420140323\n",
      "Epoch: 0484 loss= 0.419994446\n",
      "Epoch: 0485 loss= 0.419849087\n",
      "Epoch: 0486 loss= 0.419704231\n",
      "Epoch: 0487 loss= 0.419559873\n",
      "Epoch: 0488 loss= 0.419416034\n",
      "Epoch: 0489 loss= 0.419272690\n",
      "Epoch: 0490 loss= 0.419129840\n",
      "Epoch: 0491 loss= 0.418987493\n",
      "Epoch: 0492 loss= 0.418845644\n",
      "Epoch: 0493 loss= 0.418704284\n",
      "Epoch: 0494 loss= 0.418563409\n",
      "Epoch: 0495 loss= 0.418423038\n",
      "Epoch: 0496 loss= 0.418283168\n",
      "Epoch: 0497 loss= 0.418143776\n",
      "Epoch: 0498 loss= 0.418004876\n",
      "Epoch: 0499 loss= 0.417866468\n",
      "Epoch: 0500 loss= 0.417728548\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "batch_size=32\n",
    "training_epochs=500\n",
    "display_step=1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        x_batches = np.array_split(x_train, total_batch)\n",
    "        y_batches = np.array_split(y_train, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, loss], \n",
    "                            feed_dict={\n",
    "                                x_: batch_x, \n",
    "                                y_: batch_y\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15877616\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy.eval({x_: x_test, y_: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.reduce_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4045834 , -1.3274912 ,  0.8956847 , -1.7651782 ,  1.0364856 ,\n",
       "        -1.2952708 , -1.5942513 , -0.47434276, -1.8660967 ,  0.23347509],\n",
       "       [-1.2535244 , -1.5511212 ,  1.009658  ,  0.21823516, -0.0832367 ,\n",
       "         2.153867  , -0.4979945 , -0.95245534,  0.43519378, -0.3843823 ],\n",
       "       [ 0.27929285, -0.40438697, -0.87659645, -0.87517154,  0.26784536,\n",
       "         1.4496769 ,  0.17188928, -0.21270004, -1.2267426 ,  0.26768878],\n",
       "       [-0.27646998, -0.45929882,  0.02620709,  0.6652723 , -0.5112964 ,\n",
       "        -0.5047536 ,  1.3499221 , -0.84636456, -0.3131396 , -0.19102888],\n",
       "       [ 0.64400893, -0.92748016, -1.8711928 ,  0.20264588, -1.4982809 ,\n",
       "        -0.49323344, -0.00744544,  0.28357384,  0.5164095 ,  1.3628272 ],\n",
       "       [-1.687928  ,  1.3751528 ,  0.07957733,  0.25146568,  2.2168996 ,\n",
       "         0.6300091 ,  1.2901114 ,  1.7603753 , -0.34151906, -0.66972876],\n",
       "       [ 0.7972903 , -0.6933182 ,  1.2402503 ,  0.06241497,  0.8922171 ,\n",
       "        -1.3946333 ,  0.44395125, -1.1306434 , -1.0392274 ,  0.49482906],\n",
       "       [-0.4072455 , -1.964267  , -0.4900571 , -1.816462  , -0.8222824 ,\n",
       "         1.8058339 , -1.3498589 ,  1.9225928 , -1.7378465 ,  1.6941619 ],\n",
       "       [ 2.0056953 ,  0.10374009,  0.94015163,  0.3511041 ,  0.6610702 ,\n",
       "         1.8926661 , -0.9394303 , -1.1591884 ,  0.8129172 ,  0.512154  ],\n",
       "       [-0.24775766,  1.0510089 ,  1.1557988 , -0.45764062, -0.9202533 ,\n",
       "        -0.825822  , -0.19665502,  0.66097367,  0.06770304,  2.8655188 ],\n",
       "       [-0.15134485, -0.26453558, -0.5986486 , -0.11830818,  1.1666487 ,\n",
       "        -0.28460833, -3.0008025 ,  1.7590314 ,  0.37640044,  0.8501202 ],\n",
       "       [-0.03302801,  1.6746799 ,  0.28265944,  1.2223314 ,  1.1629186 ,\n",
       "        -1.834773  , -1.4476384 ,  0.41230863, -0.4099706 ,  1.6690699 ],\n",
       "       [-0.02846339, -1.1624109 , -1.5335835 , -0.14936784, -0.4164691 ,\n",
       "        -0.55936205,  0.08255074,  0.80952334, -0.6071191 ,  0.37490803],\n",
       "       [ 0.1311816 ,  1.0401019 ,  0.26239645,  0.14197022,  0.24026635,\n",
       "         2.1263614 ,  0.2872275 , -0.64952105, -0.21444409,  0.795039  ],\n",
       "       [-0.26243973, -0.34194452, -0.19293486, -0.8735266 ,  0.0060178 ,\n",
       "         0.06835473,  0.23093931, -0.21283193, -0.00926627,  1.6979648 ],\n",
       "       [ 0.66109973, -0.34086782,  1.4180608 ,  1.0944645 , -0.8654616 ,\n",
       "         0.6002254 , -0.06614711,  2.5593395 ,  0.07398107,  0.01788671],\n",
       "       [ 0.01785259,  1.7478701 ,  1.0533228 , -0.11498673, -0.66572833,\n",
       "        -0.37650508, -0.5103699 ,  0.92831105,  1.146133  , -0.74283546],\n",
       "       [ 0.70182455, -0.15690325,  1.3526242 , -0.01981128, -0.05077044,\n",
       "         0.7806222 ,  0.7597177 ,  0.77284664,  0.5498463 ,  0.9709162 ],\n",
       "       [ 0.8384002 , -0.41414574,  0.10042954, -0.07741876,  0.19828296,\n",
       "        -0.19861263,  0.591078  ,  0.16061772, -0.5128206 , -0.8552464 ],\n",
       "       [ 0.50390106, -0.5974926 ,  0.23658033, -0.5029059 , -1.3609333 ,\n",
       "         1.9074554 , -1.5413562 , -0.25487933, -0.15672363,  2.6881187 ],\n",
       "       [ 1.3011197 , -1.9031935 , -1.8447986 , -0.8997886 ,  0.8176548 ,\n",
       "         1.8969475 ,  0.8040804 , -1.6438748 ,  0.5413053 , -0.10186418],\n",
       "       [ 0.849775  , -1.0799572 ,  0.93767434,  0.59206796,  0.11286423,\n",
       "        -0.5218372 ,  0.70122707,  0.1229414 , -2.5071425 , -0.31355268],\n",
       "       [-0.1688841 ,  1.7773284 ,  0.93896526, -1.4888208 , -0.45878464,\n",
       "         2.185992  , -1.3041626 ,  2.2333205 ,  0.68700576, -0.81820065],\n",
       "       [ 0.02079119,  1.4923534 ,  0.65696037, -0.37020308, -0.6797282 ,\n",
       "        -0.55413723, -0.70516926, -1.3491194 ,  0.08357239,  1.1059183 ],\n",
       "       [-0.32799864, -2.0383022 ,  2.1558597 , -1.9090738 ,  1.3749138 ,\n",
       "         1.2194424 , -0.37452826,  1.0334976 ,  0.8109377 , -0.47278243],\n",
       "       [-0.7686079 ,  0.33136842, -0.4468593 ,  0.07233839, -0.6067968 ,\n",
       "        -1.7131087 , -0.26885384,  0.9617449 ,  0.62839264,  0.7579186 ],\n",
       "       [ 0.7836776 , -0.6490326 ,  1.1622956 ,  0.69833106,  0.21161978,\n",
       "         0.7584223 , -0.95444024,  1.2478528 , -1.4383926 ,  0.43078586],\n",
       "       [-0.9632829 , -1.5268972 , -0.5204357 , -0.08085958,  0.22433591,\n",
       "        -0.269701  ,  0.9307753 , -1.045     , -0.6180746 ,  1.4210595 ],\n",
       "       [-0.968134  ,  0.9352069 ,  1.5135409 ,  0.2895197 ,  1.2033807 ,\n",
       "         2.1870189 ,  1.1399816 ,  1.6431496 , -1.0416862 , -0.6219913 ],\n",
       "       [-2.966607  , -0.76294565, -1.4589511 ,  0.30799258,  1.4148508 ,\n",
       "         0.52564454,  0.70777637,  0.820471  ,  0.6595499 ,  0.4739465 ],\n",
       "       [-0.02826614, -1.0567396 , -1.1130674 , -0.44274846,  0.40159503,\n",
       "         0.44005656, -1.4293348 ,  0.43421426, -0.2981887 ,  0.91874564],\n",
       "       [-0.04948235,  1.1767997 ,  1.0002828 ,  1.322359  , -2.7191753 ,\n",
       "         0.3789498 , -0.12524761, -0.6519676 ,  1.0448176 ,  0.31966752],\n",
       "       [-0.169069  ,  0.2606035 ,  0.9549642 ,  0.14681412, -1.0949388 ,\n",
       "         0.97875595,  0.48199546,  0.46554115,  1.2697213 , -0.5999949 ],\n",
       "       [-0.76915157,  1.5058314 ,  1.5181258 , -1.6046754 ,  0.21363142,\n",
       "         1.3919919 , -1.3148984 ,  1.8000243 ,  1.8021164 ,  0.3075269 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
